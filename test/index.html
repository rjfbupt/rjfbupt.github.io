<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Embodied Task Planning with Large Language and Vision Models">
    <meta name="author" content="Xiuwei Xu,
                                 Zhihao Sun,
                                 Ziwei Wang,
                                 Hongmin Liu,
                                 Jie Zhou,
                                 Jiwen Lu">

    <title>Embodied Task Planning with Large Language and Vision Models</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Embodied Task Planning with Large Language and Vision Models</h2>
    <h3>CoRL 2023</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p>
        <span style="white-space: nowrap; font-size:larger">
        <a href="https://xuxw98.github.io/">Xiuwei Xu</a><sup>1,2</sup>&nbsp;&nbsp;
        Zhihao Sun<sup>3</sup>&nbsp;&nbsp;
        <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a><sup>1,2</sup>&nbsp;&nbsp;
        Hongmin Liu<sup>3</sup>&nbsp;&nbsp;
        <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a><sup>1,2</sup>&nbsp;&nbsp;
        <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a><sup>1,2&#8224;</sup>
        </span>
        <br><br>
        <sup>1</sup>Department of Automation, Tsinghua University<br>
        <sup>2</sup>Beijing National Research Center for Information Science and Technology, China<br>
        <sup>3</sup>The School of Intelligence Science and Technology, University of Science and Technology Beijing<br>
        <br><br>
        <a href="https://arxiv.org/abs/2305.03716" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/file.png" alt="paper" style="vertical-align: middle;">
            &nbsp;Paper (arXiv)
        </a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://github.com/xuxw98/DSPDet3D" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/github.png" alt="code" style="vertical-align: middle;">
            &nbsp;Code (GitHub)
        </a>
    </p>

    <!-- <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2006.09661">Paper</a>
        <a class="btn btn-primary" href="https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb">Colab Notebook</a>
        <a class="btn btn-primary" href="https://dcato98.github.io/playground/#activation=sine">Tensorflow Playground</a>
        <a class="btn btn-primary" href="https://github.com/vsitzmann/siren">Code</a>
        <a class="btn btn-primary" href="https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K">Data</a>
    </div> -->
</div>

<div class="container">
    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
            Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments.Recent large language models (LLM) can embed rich semantic knowledge for the agent in task plan generation, while they lack the information about the realistic world and usually predict infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing the triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the names of detected objects in the scenes for GPT-4 to generate a large number of instructions and corresponding planned action steps. The generated data is leveraged for grounded plan tuning of pre-trained large language models. During inference, we discover the objects in the scenes by extending open-vocabulary object detectors to multi-view RGB images collected in different locations. Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-4 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/demo.gif" alt="pipeline" width="100%">
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Approach</h2>
        <hr>
        <p>
            Through in-depth analysis, we summarize three key points for designing an effective and efficient 3D detector for small object detection: (1) multi-level FPN-like architecture; (2) increasing the spatial resolution; (3) removing the useless computation in upsampling layers. To this end, we propose DSPDet3D.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/pipeline.jpg" alt="pipeline" width="90%">
            </div>
        </div>
        <p>
            <b>Illustration of DSPDet3D.</b> The voxelized point clouds are fed into a high-resolution sparse convolutional backbone, which output four levels of scene representations. Four dynamic spatial pruning (DSP) blocks are stacked to construct a multi-level FPN-like decoder and detect objects from coarse to fine. Each DSP block merges the backbone and upsampled features, generates object proposals for detection and selectively upsamples the feature map by pruning uninformative voxels. During training, we switch the pruning to weak mode for context preservation. We detail DSP block on the right. Note that the part (a) and (c) of DSP are absent in level 4 and level 1 respectively.
        </p>
    </div>

    <div class="section">
        <h2>Experiments</h2>
        <hr>
        <p>
            We conduct extensive experiments in a simulation environment (AI2THOR) to validate the proposed approach.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/experiment.png" alt="pipeline" width="90%">
            </div>
        </div>
        <p>
            The average execution success rate of instructions on 20 validation scenes in the AI2THOR simulation environment. Each validation scene contains on average 3 instructions with GPT-3.5 generated and executable actions, for a total of 60 validation samples. Where G represents the grid size of the cruise setup, D represents the robotic look-around step length, N represents the number or percent of randomly selected points, Overall Center represents the robot is located at the center of the scene only, Partial Center represents the robot only traverses the center of a regular rectangle divided according to the scene layout.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/experiment2.png" alt="pipeline" width="90%">
            </div>
        </div>
        <p>
            We compare our approach with popular and state-of-the-art task planning. As shown above, our method achieves better performance in Incomplete and Hallucination. For example, our method decreases 71.67% in Incompletes compared to Llma; in Hallucination, our method decreases 26.67% compared to Llava.
        </p>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
            <div class="bibtexsection">
        @article{xu2023dsp, 
            title={DSPDet3D: Dynamic Spatial Pruning for 3D Small Object Detection}, 
            author={Xiuwei Xu and Zhihao Sun and Ziwei Wang and Hongmin Liu and Jie Zhou and Jiwen Lu},
            journal={arXiv preprint arXiv:2305.03716},
            year={2023}
        }
            </div>
        </div>
    </div>

    <hr>

    <footer>
        <!-- <h6>Acknowledgement</h6> -->
        <p><small>The website template was borrowed from <a href="https://xuxw98.github.io/DSPDet3D/">Xiuwei Xu</a></small></p>
    </footer>
</div>

    <p><center>
        <div id="clustrmaps-widget" style="width:30%">
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=KFxiSzNXdwhkw0Znzcat5j6WJhvziaXtw3yEgmx6Y8c&cl=ffffff&w=a"></script>
        </div>        
        <br>
        &copy; Jiangfan Ran | Last update: June. 13, 2023
    </center></p>

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
