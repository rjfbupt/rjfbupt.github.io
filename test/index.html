<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Embodied Task Planning with Large Language and Vision Models">
    <meta name="author" content="Xiuwei Xu,
                                 Zhihao Sun,
                                 Ziwei Wang,
                                 Hongmin Liu,
                                 Jie Zhou,
                                 Jiwen Lu">

    <title>Embodied Task Planning with Large Language and Vision Models</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Embodied Task Planning with Large Language and Vision Models</h2>
    <h3>arXiv 2023</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p>
        <span style="white-space: nowrap; font-size:larger">
        <a href="https://xuxw98.github.io/">Xiuwei Xu</a><sup>1,2</sup>&nbsp;&nbsp;
        Zhihao Sun<sup>3</sup>&nbsp;&nbsp;
        <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a><sup>1,2</sup>&nbsp;&nbsp;
        Hongmin Liu<sup>3</sup>&nbsp;&nbsp;
        <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a><sup>1,2</sup>&nbsp;&nbsp;
        <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a><sup>1,2&#8224;</sup>
        </span>
        <br><br>
        <sup>1</sup>Department of Automation, Tsinghua University<br>
        <sup>2</sup>Beijing National Research Center for Information Science and Technology, China<br>
        <sup>3</sup>The School of Intelligence Science and Technology, University of Science and Technology Beijing<br>
        <br><br>
        <a href="https://arxiv.org/abs/2305.03716" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/file.png" alt="paper" style="vertical-align: middle;">
            &nbsp;Paper (arXiv)
        </a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://github.com/xuxw98/DSPDet3D" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/github.png" alt="code" style="vertical-align: middle;">
            &nbsp;Code (GitHub)
        </a>
    </p>

    <!-- <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2006.09661">Paper</a>
        <a class="btn btn-primary" href="https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb">Colab Notebook</a>
        <a class="btn btn-primary" href="https://dcato98.github.io/playground/#activation=sine">Tensorflow Playground</a>
        <a class="btn btn-primary" href="https://github.com/vsitzmann/siren">Code</a>
        <a class="btn btn-primary" href="https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K">Data</a>
    </div> -->
</div>

<div class="container">
    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
            Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments.Recent large language models (LLM) can embed rich semantic knowledge for the agent in task plan generation, while they lack the information about the realistic world and usually predict infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing the triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the names of detected objects in the scenes for GPT-4 to generate a large number of instructions and corresponding planned action steps. The generated data is leveraged for grounded plan tuning of pre-trained large language models. During inference, we discover the objects in the scenes by extending open-vocabulary object detectors to multi-view RGB images collected in different locations. Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-4 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/demo.gif" alt="pipeline" width="100%">
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Approach</h2>
        <hr>
        <p>
            Although multi-modal VLMs have achieved surprising performance on a wide range of fields, embodied task planning still remains a challenging task due to: 1) the lack of relevant datasets; 2) the requirement of simultaneous scene understanding and reasoning.Considering the recent success of GPT models on high-level human-like reasoning, we propose to represent the embodied scenes with texts and leverage ChatGPT/GPT-4 for data generation.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/pipeline.jpg" alt="pipeline" width="90%">
            </div>
        </div>
        <p>
            <b>Illustration of TaPA.</b> Given an embodied 3D scene,we divide the scene into regular rectangles based on the room layout. In this approach, we traverse solely the centroids of each rectangle and perform look-around observations. The number of circumferential views depends on the set angle, which is 360 divided by the set angle. Then we will perform open vocabulary object perception through Detic and integrate the recognized objects to obtain the caption of scene.Finally TaPA generates task planning based on the user's question.
        </p>
    </div>

    <div class="section">
        <h2>Experiments</h2>
        <hr>
        <p>
            We conduct extensive experiments in a simulation environment (AI2THOR) to validate the proposed approach.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/experiment2.png" alt="pipeline" width="90%">
            </div>
        </div>
        <p>
            We compare our approach with popular and state-of-the-art mission planning. As shown above, our method has better performance compared to the previous methods. For example, we improve 11.92% compared to GPT-3.5.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/experiment1.png" alt="pipeline" width="90%">
            </div>
        </div>
        <p>
            We also compared the operational rationality and planning rationality with the previous method. As shown above, our method achieves better performance in Incomplete and Hallucination. For example, our method decreases 71.67% in Incompletes compared to Llma; in Hallucination, our method decreases 26.67% compared to Llava.
        </p>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
            <div class="bibtexsection">
        @article{xu2023dsp, 
            title={DSPDet3D: Dynamic Spatial Pruning for 3D Small Object Detection}, 
            author={Xiuwei Xu and Zhihao Sun and Ziwei Wang and Hongmin Liu and Jie Zhou and Jiwen Lu},
            journal={arXiv preprint arXiv:2305.03716},
            year={2023}
        }
            </div>
        </div>
    </div>

    <hr>

    <footer>
        <!-- <h6>Acknowledgement</h6> -->
        <p><small>The website template was borrowed from <a href="https://xuxw98.github.io/DSPDet3D/">Xiuwei Xu</a></small></p>
    </footer>
</div>

    <p><center>
        <div id="clustrmaps-widget" style="width:30%">
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=KFxiSzNXdwhkw0Znzcat5j6WJhvziaXtw3yEgmx6Y8c&cl=ffffff&w=a"></script>
        </div>        
        <br>
        &copy; Jiangfan Ran | Last update: June. 14, 2023
    </center></p>

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
